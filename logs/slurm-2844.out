INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:root:called-params configs/in1k_vith14_ep300_FGDCC.yaml
INFO:root:loaded params...
{   'data': {   'batch_size': 96,
                'color_jitter_strength': 0.0,
                'crop_scale': [0.3, 1.0],
                'crop_size': 224,
                'cutmix': 0.0,
                'drop_path': 0.25,
                'image_folder': '/home/rtcalumby/adam/luciano/plantnet_300K/',
                'mixup': 0.0,
                'nb_classes': 1081,
                'num_workers': 8,
                'pin_mem': True,
                'reprob': 0.25,
                'resume_epoch': 0,
                'root_path': '/home/rtcalumby/adam/luciano/',
                'use_color_distortion': False,
                'use_gaussian_blur': False,
                'use_horizontal_flip': False},
    'logging': {   'folder': '/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/logs/PlantNet300k_exp21',
                   'write_tag': 'jepa'},
    'mask': {   'allow_overlap': False,
                'aspect_ratio': [0.75, 1.5],
                'enc_mask_scale': [0.85, 1.0],
                'min_keep': 10,
                'num_enc_masks': 1,
                'num_pred_masks': 4,
                'patch_size': 14,
                'pred_mask_scale': [0.15, 0.2]},
    'meta': {   'copy_data': False,
                'load_checkpoint': True,
                'model_name': 'vit_huge',
                'pred_depth': 12,
                'pred_emb_dim': 384,
                'read_checkpoint': None,
                'use_bfloat16': True},
    'optimization': {   'ema': [0.9, 0.999],
                        'epochs': 50,
                        'final_lr': 3e-06,
                        'final_weight_decay': 0.4,
                        'ipe_scale': 1.0,
                        'label_smoothing': 0.1,
                        'lr': 0.0006,
                        'start_lr': 0.00015,
                        'warmup': 5,
                        'weight_decay': 0.05}}
INFO:root:Running... (rank: 0/1)
INFO:root:Initialized (rank/world-size) 0/1
INFO:root:MaskedAutoEncoder(
  (encoder): Sequential(
    (0): Linear(in_features=1024, out_features=768, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=768, out_features=512, bias=True)
    (3): GELU(approximate='none')
    (4): Linear(in_features=512, out_features=384, bias=True)
    (5): GELU(approximate='none')
  )
  (decoder): Sequential(
    (0): Linear(in_features=384, out_features=512, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=512, out_features=768, bias=True)
    (3): GELU(approximate='none')
    (4): Linear(in_features=768, out_features=1024, bias=True)
  )
)
INFO:root:making imagenet data transforms
INFO:root:making imagenet data transforms
INFO:root:Finetuning dataset created
Training dataset, length: 245952
INFO:root:Finetuning dataset created
Val dataset, length: 31200
INFO:root:Using AdamW
['encoder', 'predictor', 'opt', 'scaler', 'target_encoder', 'epoch', 'loss', 'batch_size', 'world_size', 'lr']
INFO:root:loaded pretrained encoder from epoch 66 with msg: <All keys matched successfully>
INFO:root:Using AdamW
INFO:root:VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14))
  )
  (blocks): ModuleList(
    (0-31): 32 x Block(
      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1280, out_features=3840, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1280, out_features=1280, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=1280, out_features=5120, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=5120, out_features=1280, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
)
INFO:root:Building cache...
INFO:root:Done.
INFO:root:Initializing centroids...
INFO:root:Done.
INFO:root:M - Step...
/home/rtcalumby/miniconda3/envs/py382/lib/python3.12/site-packages/faiss/contrib/torch_utils.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  x.storage().data_ptr() + x.storage_offset() * 4)
Xb Current device: -1
FAISS K-MEANS (on CPU execution): 2.0083866119384766 seconds
OUR K-MEANS (on GPU execution): 4.872987270355225 seconds
GPU runtime gain: 2.426319335823426
Qualitative Results:
FAISS centroid 0: [ 7.94545785e-02 -1.62534356e-01  3.14762324e-01 -7.92384684e-01
 -2.67671287e-01 -1.40844536e+00 -1.92676961e-01 -3.19253236e-01
 -1.86381321e-02  3.36449772e-01  1.29656613e-01  1.79481670e-01
 -6.20734215e-01  5.56551991e-03  1.29596639e+00  3.40409815e-01
 -5.99548399e-01  6.03997886e-01 -4.99200791e-01  1.03639710e+00
 -5.12818635e-01 -4.26834337e-02 -5.39150476e-01 -7.60362506e-01
  1.52398139e-01 -6.13800704e-01 -3.47699702e-01 -9.57664922e-02
  5.99374771e-01 -4.85301197e-01  4.83264595e-01 -2.19405755e-01
  1.03730702e+00  1.26722372e+00 -8.53269339e-01  5.40236950e-01
  8.68760645e-01 -8.37709248e-01  1.81715751e+00  8.76160115e-02
  2.39980400e-01  6.61011577e-01 -2.48283774e-01 -1.72948742e+00
  7.76722968e-01 -4.99233216e-01  3.45726550e-01 -6.95415512e-02
  1.62348762e-01 -3.68860930e-01  1.07020175e+00 -5.50577104e-01
  5.05105674e-01  1.21847309e-01 -7.54262626e-01 -2.30041116e-01
 -1.08251119e+00  3.16748261e-01  3.63802075e-01 -6.47813007e-02
 -6.20332181e-01  5.50858676e-01 -1.78678918e+00 -7.50703454e-01
  8.82457972e-01  3.79792042e-02 -4.65226233e-01 -1.56564936e-01
  4.07235086e-01 -5.22179902e-01  3.98617983e-01 -1.04877186e+00
  5.14267050e-02  3.08602363e-01 -3.01438689e-01  7.49449611e-01
 -1.00662559e-01 -2.58142799e-01 -2.76330560e-01 -5.09837449e-01
 -2.59037465e-01  1.49547502e-01 -6.50083840e-01  3.30861956e-01
  3.95751089e-01 -4.08551574e-01  4.45966482e-01 -8.04947257e-01
 -1.59376830e-01 -4.22773570e-01 -2.42901266e-01  2.62999624e-01
 -6.91133514e-02  1.53320953e-01 -2.95926988e-01 -5.78707039e-01
 -8.12601596e-02  1.71040356e-01  3.48027706e-01  1.13104366e-01
  1.08482465e-01  1.04286037e-01  3.15061092e-01 -1.79851636e-01
  5.84185123e-01 -5.43811917e-02  5.02968431e-01 -2.77151287e-01
  1.21503103e+00 -3.83382142e-01 -1.96124852e-01 -2.75909066e-01
  4.04308289e-01  8.06574106e-01 -2.48429239e-01 -1.05019495e-01
 -1.33417118e+00  5.72018683e-01 -2.34865174e-02 -5.86061180e-01
  6.64526105e-01 -5.89900374e-01  8.03800464e-01  1.72017586e+00
 -1.88929006e-01 -2.50887036e-01 -6.37077987e-01 -7.39728034e-01
  4.24404562e-01 -3.57673109e-01 -3.64794463e-01  1.34437243e-02
 -7.47787774e-01 -9.33606386e-01 -3.28111798e-01 -4.91019413e-02
 -9.22119692e-02  5.75077355e-01  9.98376161e-02  9.96822596e-01
 -6.70284331e-01  3.17280948e-01 -2.79768053e-02  1.21685147e+00
 -2.52969682e-01 -2.56747961e-01 -2.47382730e-01 -2.54210681e-01
  2.52236485e-01  3.83411556e-01 -5.80533855e-02  2.01225728e-01
  9.14642274e-01  8.57301712e-01  5.22393510e-02  1.32683134e+00
 -3.30936089e-02 -1.57146466e+00 -5.20535648e-01  3.86121511e-01
 -2.07726464e-01 -6.68534994e-01  6.97416008e-01 -4.65518445e-01
  1.67559355e-01 -4.22663540e-01  3.01563442e-01  7.75163472e-02
  4.16530788e-01  8.66854310e-01  3.50619525e-01 -6.99708983e-02
 -8.60027969e-01 -5.25708258e-01  1.00759290e-01 -1.76078707e-01
  3.04402318e-02 -4.23605084e-01  3.60486768e-02 -9.55105484e-01
 -1.30543983e+00 -9.41369295e-01  4.81743157e-01  4.95944709e-01
  3.85672739e-03  5.58067381e-01  6.96828425e-01  3.19461405e-01
  8.67429450e-02 -5.32230735e-01 -3.92732546e-02 -5.95199727e-02
  3.81265283e-01  5.90570152e-01 -1.31710142e-01 -7.76146710e-01
  5.15584290e-01 -2.99595863e-01  9.01927292e-01  2.92493999e-01
  4.28017616e-01  5.71995899e-02 -3.35413873e-01 -5.42133093e-01
 -3.10214251e-01  3.54084164e-01  4.53097790e-01 -5.14624864e-02
  1.21653907e-01 -9.32626612e-03 -2.43753001e-01 -3.13520998e-01
 -7.85839617e-01 -6.99463859e-02 -2.52627611e-01 -9.72925499e-02
  2.06498224e-02 -4.90043133e-01  6.72673404e-01 -3.17291528e-01
 -6.63936496e-01  1.32909700e-01 -5.46852291e-01  2.69560546e-01
  1.75083190e-01  3.10312718e-01 -1.68795720e-01  3.54935884e-01
  2.14279667e-01  1.05437987e-01  5.46632148e-02 -3.30913752e-01
 -4.05247748e-01  7.29314089e-01  8.49524021e-01 -2.10162178e-01
 -1.83623329e-01 -1.87105730e-01  1.50227427e+00  9.22795773e-01
  4.25780535e-01 -1.15507340e+00 -5.14230467e-02  6.77677035e-01
 -1.87011972e-01 -1.27010763e+00 -5.50108373e-01 -8.28935742e-01
 -7.52658444e-03 -6.73882589e-02  6.12944484e-01  1.13401823e-01
  6.02966964e-01 -8.71637702e-01  4.83483374e-02  3.07584703e-01
  1.50513780e+00  1.21469188e+00 -4.47888017e-01 -6.12753183e-02
 -8.03578794e-01 -1.74447930e+00  2.17331126e-01  1.47714950e-02
 -2.01974943e-01 -3.51521909e-01  2.49837860e-01  1.20228183e+00
  4.58739586e-02 -1.16591349e-01 -4.30954188e-01  9.33635384e-02
  4.00975853e-01 -1.39676318e-01 -4.92787927e-01 -8.37926149e-01
  1.43321669e+00 -6.79145336e-01  2.59695679e-01  4.21794914e-02
  9.60434880e-03  1.03040314e+00  4.11619812e-01 -8.49894404e-01
  8.77567589e-01  3.04626286e-01 -1.35434449e-01  2.46445403e-01
  1.33021522e+00  6.29806459e-01 -7.91463852e-02 -1.08994581e-01
  9.70826447e-01  5.24754226e-01 -4.01381969e-01 -5.92071339e-02
 -5.90810120e-01 -6.72263324e-01 -9.59445715e-01 -7.03427970e-01
 -3.31562199e-02  5.34650266e-01 -6.08606935e-01  7.06543565e-01
 -7.49522269e-01 -7.05052674e-01 -4.84125704e-01 -6.68871522e-01
  4.37595814e-01 -5.69344997e-01 -7.70646706e-02  1.16781771e+00
  7.39320070e-02 -1.42266154e-02  2.14935660e-01 -3.13667417e-01
 -2.07917213e-01 -1.28969951e-02  1.59426510e-01 -7.40875840e-01
  1.31891418e+00 -1.42784977e+00 -3.79563749e-01 -4.20728326e-01
  5.07510006e-01 -6.93091080e-02  1.65340471e+00 -5.31131446e-01
  6.15679324e-01 -4.75036085e-01  1.04900455e+00 -9.38868499e-04
  2.39273146e-01  4.39608335e-01 -1.05473244e+00  4.96373862e-01
 -7.58378804e-01 -1.62057355e-01  6.27932727e-01  8.07720006e-01
  1.85505226e-01  2.62775928e-01 -2.11184964e-01  1.18180834e-01
 -9.19282913e-01  4.99366254e-01  4.17739190e-02  1.60409343e+00
 -4.08778727e-01  6.44674748e-02 -1.75678086e+00  1.62324858e+00
 -9.69980776e-01  2.89758086e-01 -1.30518422e-01  1.00711131e+00
  3.56405079e-01  2.97703296e-01 -3.98524702e-01  1.29833370e-01
 -2.39223838e-01 -8.00082207e-01 -3.61828059e-01 -4.20337260e-01
 -1.73513040e-01  1.37361363e-01 -2.40061134e-01 -3.25986177e-01
  6.13064706e-01  2.10295394e-01  1.10778773e+00  4.47548091e-01
 -3.54475647e-01  6.17179751e-01  1.31033659e-01 -1.09185040e+00
  2.61912402e-02 -3.44800264e-01 -9.22185034e-02  9.14889723e-02
 -1.33023411e-01 -5.23291975e-02  5.13753621e-03  3.80178422e-01]
our centroid 0: Process Process-1:
Traceback (most recent call last):
  File "/home/rtcalumby/miniconda3/envs/py382/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/rtcalumby/miniconda3/envs/py382/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/DeepCluster/main_FGDCC.py", line 52, in process_main
    app_main(args=params)
  File "/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/DeepCluster/engine_FGDCC.py", line 455, in main
    M_losses = k_means_module.update(cached_features_last_epoch, device, empty_clusters_per_epoch) # M-step
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/DeepCluster/src/KMeans.py", line 245, in update
    print((ours_centroid_0 - cluster_centroids[0][0]))
           ~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~
  File "/home/rtcalumby/miniconda3/envs/py382/lib/python3.12/site-packages/torch/_tensor.py", line 1087, in __array__
    return self.numpy()
           ^^^^^^^^^^^^
TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
tensor([ 8.5770e-02, -1.5593e-01,  3.0013e-01, -7.6068e-01, -2.3959e-01,
        -1.4119e+00, -1.6859e-01, -3.0909e-01, -3.0201e-02,  3.3230e-01,
         1.2992e-01,  1.5193e-01, -6.0349e-01, -1.0024e-02,  1.2737e+00,
         3.6650e-01, -6.0273e-01,  5.8682e-01, -4.9638e-01,  1.0536e+00,
        -5.3451e-01, -3.9686e-02, -5.2384e-01, -7.5571e-01,  1.6146e-01,
        -6.3949e-01, -3.3400e-01, -9.2286e-02,  5.5943e-01, -4.9383e-01,
         4.7983e-01, -2.0416e-01,  1.0497e+00,  1.2543e+00, -8.6996e-01,
         5.4701e-01,  8.6505e-01, -8.3707e-01,  1.7729e+00,  1.1329e-01,
         2.4823e-01,  6.6901e-01, -2.5407e-01, -1.6948e+00,  7.6824e-01,
        -4.9504e-01,  3.5053e-01, -7.2503e-02,  1.4804e-01, -3.5638e-01,
         1.0680e+00, -5.5575e-01,  5.1319e-01,  1.2642e-01, -7.4349e-01,
        -2.3097e-01, -1.0932e+00,  3.1419e-01,  3.7348e-01, -7.0515e-02,
        -6.1814e-01,  5.4386e-01, -1.7901e+00, -7.4908e-01,  8.8844e-01,
         5.1708e-02, -4.5289e-01, -1.8852e-01,  3.8499e-01, -5.2703e-01,
         3.6372e-01, -1.0322e+00,  2.0130e-02,  2.9752e-01, -3.0739e-01,
         7.7889e-01, -8.0256e-02, -2.5422e-01, -2.8819e-01, -4.9666e-01,
        -2.4552e-01,  1.5588e-01, -6.5861e-01,  3.2816e-01,  4.3000e-01,
        -4.3423e-01,  4.3824e-01, -7.9000e-01, -1.4930e-01, -4.2875e-01,
        -2.4496e-01,  2.4494e-01, -4.3236e-02,  1.7447e-01, -3.3171e-01,
        -5.8078e-01, -7.5103e-02,  1.6804e-01,  3.2229e-01,  1.6084e-01,
         8.6319e-02,  1.1443e-01,  3.4517e-01, -2.1766e-01,  5.7370e-01,
        -7.5042e-02,  5.0015e-01, -2.6763e-01,  1.1663e+00, -3.5826e-01,
        -1.8750e-01, -2.9079e-01,  3.8121e-01,  7.8680e-01, -2.4016e-01,
        -1.3172e-01, -1.3069e+00,  5.6206e-01,  8.7527e-03, -6.0245e-01,
         6.6362e-01, -5.9555e-01,  7.7273e-01,  1.7090e+00, -1.7916e-01,
        -2.4620e-01, -6.4141e-01, -7.4108e-01,  4.2257e-01, -3.6087e-01,
        -3.6889e-01, -2.4267e-02, -7.5490e-01, -9.6473e-01, -3.3843e-01,
        -2.6348e-02, -1.1815e-01,  5.6909e-01,  1.0071e-01,  1.0294e+00,
        -6.7745e-01,  3.2125e-01, -2.8573e-02,  1.1901e+00, -2.4272e-01,
        -2.3325e-01, -2.6929e-01, -2.3920e-01,  2.6747e-01,  3.9173e-01,
        -6.0523e-02,  2.1901e-01,  9.1181e-01,  8.0496e-01,  2.2259e-02,
         1.3165e+00, -3.8517e-02, -1.5562e+00, -5.2428e-01,  4.2414e-01,
        -2.2005e-01, -6.8257e-01,  7.0827e-01, -4.6941e-01,  1.7291e-01,
        -4.3065e-01,  2.9579e-01,  6.8839e-02,  3.8192e-01,  8.0326e-01,
         3.5337e-01, -4.1814e-02, -8.6389e-01, -5.2366e-01,  9.7912e-02,
        -1.6335e-01,  3.2013e-02, -4.1336e-01,  2.7371e-02, -9.4400e-01,
        -1.2828e+00, -9.3371e-01,  4.4919e-01,  4.8981e-01, -2.7564e-03,
         5.7037e-01,  7.3365e-01,  2.9459e-01,  8.4941e-02, -5.1361e-01,
        -3.5065e-02, -6.5765e-02,  3.7005e-01,  5.8701e-01, -1.2769e-01,
        -7.6160e-01,  5.0923e-01, -2.9984e-01,  9.1363e-01,  2.8709e-01,
         4.3291e-01,  7.3440e-02, -3.4246e-01, -5.4423e-01, -3.0257e-01,
         3.6149e-01,  4.6874e-01, -5.2408e-02,  1.1683e-01, -6.8112e-03,
        -2.6460e-01, -3.1814e-01, -7.7132e-01, -1.1863e-01, -2.3635e-01,
        -8.1054e-02,  6.9023e-03, -4.7530e-01,  6.7166e-01, -3.1080e-01,
        -6.5777e-01,  1.4816e-01, -5.2603e-01,  2.7991e-01,  1.9239e-01,
         3.0680e-01, -1.4731e-01,  3.6743e-01,  1.9638e-01,  8.4083e-02,
         7.4225e-02, -3.3666e-01, -3.9898e-01,  6.7323e-01,  8.4380e-01,
        -2.1701e-01, -1.8612e-01, -1.8831e-01,  1.4940e+00,  9.1660e-01,
         4.3135e-01, -1.1465e+00, -7.9083e-02,  6.8714e-01, -1.7322e-01,
        -1.2483e+00, -5.0865e-01, -8.2598e-01,  3.2173e-04, -6.1571e-02,
         6.0009e-01,  9.2871e-02,  6.0497e-01, -8.5330e-01,  1.7470e-02,
         3.2288e-01,  1.4668e+00,  1.1988e+00, -4.8193e-01, -7.0560e-02,
        -7.8263e-01, -1.7411e+00,  1.9912e-01,  2.1745e-02, -1.9348e-01,
        -3.4720e-01,  2.3319e-01,  1.1896e+00,  3.6407e-02, -1.2615e-01,
        -4.5974e-01,  8.7829e-02,  4.1522e-01, -1.3077e-01, -5.0323e-01,
        -8.1145e-01,  1.4152e+00, -7.0075e-01,  2.9392e-01,  2.2416e-02,
         2.9458e-03,  1.0127e+00,  3.8764e-01, -8.4738e-01,  8.7535e-01,
         2.6226e-01, -1.4322e-01,  2.6140e-01,  1.3324e+00,  6.1101e-01,
        -6.9747e-02, -8.9419e-02,  9.6544e-01,  4.9784e-01, -3.9123e-01,
        -8.0199e-02, -6.1349e-01, -6.3024e-01, -9.2835e-01, -7.0711e-01,
        -3.9878e-02,  5.1423e-01, -5.9202e-01,  7.2185e-01, -7.4628e-01,
        -6.8774e-01, -4.6723e-01, -6.4951e-01,  4.2679e-01, -5.4555e-01,
        -7.4889e-02,  1.1528e+00,  5.6063e-02, -1.4334e-02,  2.1004e-01,
        -3.0682e-01, -1.8586e-01, -1.9747e-02,  1.5246e-01, -7.5565e-01,
         1.2766e+00, -1.4144e+00, -3.6051e-01, -4.3871e-01,  5.3205e-01,
        -5.5835e-02,  1.6642e+00, -5.0814e-01,  6.1684e-01, -4.3356e-01,
         1.0680e+00, -3.4264e-02,  2.1772e-01,  4.3852e-01, -1.0473e+00,
         4.8087e-01, -7.6228e-01, -1.8219e-01,  6.4573e-01,  8.1810e-01,
         1.9255e-01,  2.6674e-01, -2.0617e-01,  9.3055e-02, -8.9630e-01,
         4.9429e-01,  4.2661e-02,  1.5892e+00, -3.7808e-01,  2.9079e-02,
        -1.7044e+00,  1.6264e+00, -9.5443e-01,  2.9599e-01, -1.0926e-01,
         1.0120e+00,  3.3313e-01,  3.1387e-01, -3.8768e-01,  1.4862e-01,
        -1.9345e-01, -7.8973e-01, -3.5045e-01, -3.9469e-01, -1.8032e-01,
         1.5120e-01, -2.4178e-01, -3.1673e-01,  6.2139e-01,  1.9512e-01,
         1.1024e+00,  4.3112e-01, -3.5263e-01,  6.1600e-01,  1.4476e-01,
        -1.0424e+00,  4.1634e-02, -3.5354e-01, -7.8423e-02,  7.9539e-02,
        -1.1617e-01, -4.6245e-02,  1.5401e-02,  3.8284e-01], device='cuda:0')
