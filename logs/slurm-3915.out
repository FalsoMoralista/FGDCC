INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:root:called-params configs/in1k_vith14_ep300_FGDCC.yaml
INFO:root:loaded params...
{   'data': {   'batch_size': 96,
                'color_jitter_strength': 0.0,
                'crop_scale': [0.3, 1.0],
                'crop_size': 224,
                'cutmix': 0.0,
                'drop_path': 0.25,
                'image_folder': '/home/rtcalumby/adam/luciano/plantnet_300K/',
                'mixup': 0.0,
                'nb_classes': 1081,
                'num_workers': 16,
                'pin_mem': True,
                'reprob': 0.25,
                'resume_epoch': 0,
                'root_path': '/home/rtcalumby/adam/luciano/',
                'use_color_distortion': False,
                'use_gaussian_blur': False,
                'use_horizontal_flip': False},
    'logging': {   'folder': '/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/logs/PlantNet300k_exp40',
                   'write_tag': 'jepa'},
    'mask': {   'allow_overlap': False,
                'aspect_ratio': [0.75, 1.5],
                'enc_mask_scale': [0.85, 1.0],
                'min_keep': 10,
                'num_enc_masks': 1,
                'num_pred_masks': 4,
                'patch_size': 14,
                'pred_mask_scale': [0.15, 0.2]},
    'meta': {   'copy_data': False,
                'load_checkpoint': True,
                'model_name': 'vit_huge',
                'pred_depth': 12,
                'pred_emb_dim': 384,
                'read_checkpoint': None,
                'use_bfloat16': True},
    'optimization': {   'ema': [0.9, 0.999],
                        'epochs': 75,
                        'final_lr': 1e-05,
                        'final_weight_decay': 0.4,
                        'ipe_scale': 1.0,
                        'label_smoothing': 0.1,
                        'lr': 2.5e-05,
                        'start_lr': 8.5e-05,
                        'warmup': 5,
                        'weight_decay': 0.05}}
INFO:root:Running... (rank: 0/1)
INFO:root:Initialized (rank/world-size) 0/1
INFO:root:making imagenet data transforms
INFO:root:making imagenet data transforms
INFO:root:Finetuning dataset created
Training dataset, length: 245952
INFO:root:Finetuning dataset created
Val dataset, length: 31200
INFO:root:Using AdamW
['encoder', 'predictor', 'opt', 'scaler', 'target_encoder', 'epoch', 'loss', 'batch_size', 'world_size', 'lr']
INFO:root:loaded pretrained encoder from epoch 66 with msg: <All keys matched successfully>
INFO:root:PairedCrossAttentionClassifier(
  (act): GELU(approximate='none')
  (subclass_proj): Sequential(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): GELU(approximate='none')
  )
  (parent_cross_attention): MultiHeadCrossAttention(
    (query): Linear(in_features=1280, out_features=1280, bias=True)
    (key): Linear(in_features=1280, out_features=1280, bias=True)
    (value): Linear(in_features=1280, out_features=1280, bias=True)
  )
  (subclass_cross_attention): MultiHeadCrossAttention(
    (query): Linear(in_features=1280, out_features=1280, bias=True)
    (key): Linear(in_features=1280, out_features=1280, bias=True)
    (value): Linear(in_features=1280, out_features=1280, bias=True)
  )
  (parent_feature_selection): Linear(in_features=2560, out_features=1280, bias=True)
  (subclass_feature_selection): Linear(in_features=2560, out_features=1280, bias=True)
  (parent_classifier): Linear(in_features=1280, out_features=1081, bias=True)
  (subclass_classifier): Linear(in_features=1280, out_features=4324, bias=True)
  (head_drop): Dropout(p=0.25, inplace=False)
)
INFO:root:Using AdamW
INFO:root:VisionTransformerAutoEncoder(
  (encoder): Sequential(
    (0): Block(
      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1280, out_features=3840, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1280, out_features=1280, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=1280, out_features=1024, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1024, out_features=1280, bias=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (1): Linear(in_features=1280, out_features=1024, bias=True)
    (2): GELU(approximate='none')
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=1024, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=1024, bias=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (4): Linear(in_features=1024, out_features=768, bias=True)
  )
  (decoder): Sequential(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=1024, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1024, out_features=768, bias=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (1): Linear(in_features=768, out_features=1024, bias=True)
    (2): GELU(approximate='none')
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=1024, out_features=1280, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1280, out_features=1024, bias=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (4): Linear(in_features=1024, out_features=1280, bias=True)
  )
)
INFO:root:VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14))
  )
  (blocks): ModuleList(
    (0-31): 32 x Block(
      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1280, out_features=3840, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1280, out_features=1280, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=1280, out_features=5120, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=5120, out_features=1280, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
)
INFO:root:Autoencoder Training...
INFO:root: - - Epoch: 1 - - 
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:root:[1,     0/ 2562] - [Autoencoder Training] [Autoencoder Loss: 0.9999] [autoencoder lr: 9.00e-04][mem: 1.03e+04](2827.5 ms)
terminate called without an active exception
Process Process-1:
Traceback (most recent call last):
  File "/home/rtcalumby/miniconda3/envs/py382/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/rtcalumby/miniconda3/envs/py382/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/DeepCluster/main_FGDCC.py", line 52, in process_main
    app_main(args=params)
  File "/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/DeepCluster/engine_FGDCC.py", line 542, in main
    cached_features_last_epoch, AE_scheduler, AE_optimizer = train_autoencoder(fgdcc=fgdcc,
                                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/DeepCluster/engine_FGDCC.py", line 528, in train_autoencoder
    (ae_lr, reconstruction_loss, cache), elapsed_time = gpu_timer(train_step)
                                                        ^^^^^^^^^^^^^^^^^^^^^
  File "/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/DeepCluster/src/utils/logging.py", line 21, in gpu_timer
    result = closure()
             ^^^^^^^^^
  File "/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/DeepCluster/engine_FGDCC.py", line 507, in train_step
    reconstruction_loss, bottleneck_output, _, _ = fgdcc(x, device)
                                                   ^^^^^^^^^^^^^^^^
  File "/home/rtcalumby/miniconda3/envs/py382/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rtcalumby/miniconda3/envs/py382/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rtcalumby/miniconda3/envs/py382/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1589, in forward
    inputs, kwargs = self._pre_forward(*inputs, **kwargs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rtcalumby/miniconda3/envs/py382/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1480, in _pre_forward
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: classifier.subclass_classifier.bias, classifier.subclass_classifier.weight, classifier.parent_classifier.bias, classifier.parent_classifier.weight, classifier.subclass_feature_selection.bias, classifier.subclass_feature_selection.weight, classifier.parent_feature_selection.bias, classifier.parent_feature_selection.weight, classifier.subclass_cross_attention.value.bias, classifier.subclass_cross_attention.value.weight, classifier.subclass_cross_attention.key.bias, classifier.subclass_cross_attention.key.weight, classifier.subclass_cross_attention.query.bias, classifier.subclass_cross_attention.query.weight, classifier.parent_cross_attention.value.bias, classifier.parent_cross_attention.value.weight, classifier.parent_cross_attention.key.bias, classifier.parent_cross_attention.key.weight, classifier.parent_cross_attention.query.bias, classifier.parent_cross_attention.query.weight, classifier.subclass_proj.0.bias, classifier.subclass_proj.0.weight, vit_encoder.norm.bias, vit_encoder.norm.weight, vit_encoder.blocks.31.mlp.fc2.bias, vit_encoder.blocks.31.mlp.fc2.weight, vit_encoder.blocks.31.mlp.fc1.bias, vit_encoder.blocks.31.mlp.fc1.weight, vit_encoder.blocks.31.norm2.bias, vit_encoder.blocks.31.norm2.weight, vit_encoder.blocks.31.attn.proj.bias, vit_encoder.blocks.31.attn.proj.weight, vit_encoder.blocks.31.attn.qkv.bias, vit_encoder.blocks.31.attn.qkv.weight, vit_encoder.blocks.31.norm1.bias, vit_encoder.blocks.31.norm1.weight, vit_encoder.blocks.30.mlp.fc2.bias, vit_encoder.blocks.30.mlp.fc2.weight, vit_encoder.blocks.30.mlp.fc1.bias, vit_encoder.blocks.30.mlp.fc1.weight, vit_encoder.blocks.30.norm2.bias, vit_encoder.blocks.30.norm2.weight, vit_encoder.blocks.30.attn.proj.bias, vit_encoder.blocks.30.attn.proj.weight, vit_encoder.blocks.30.attn.qkv.bias, vit_encoder.blocks.30.attn.qkv.weight, vit_encoder.blocks.30.norm1.bias, vit_encoder.blocks.30.norm1.weight, vit_encoder.blocks.29.mlp.fc2.bias, vit_encoder.blocks.29.mlp.fc2.weight, vit_encoder.blocks.29.mlp.fc1.bias, vit_encoder.blocks.29.mlp.fc1.weight, vit_encoder.blocks.29.norm2.bias, vit_encoder.blocks.29.norm2.weight, vit_encoder.blocks.29.attn.proj.bias, vit_encoder.blocks.29.attn.proj.weight, vit_encoder.blocks.29.attn.qkv.bias, vit_encoder.blocks.29.attn.qkv.weight, vit_encoder.blocks.29.norm1.bias, vit_encoder.blocks.29.norm1.weight, vit_encoder.blocks.28.mlp.fc2.bias, vit_encoder.blocks.28.mlp.fc2.weight, vit_encoder.blocks.28.mlp.fc1.bias, vit_encoder.blocks.28.mlp.fc1.weight, vit_encoder.blocks.28.norm2.bias, vit_encoder.blocks.28.norm2.weight, vit_encoder.blocks.28.attn.proj.bias, vit_encoder.blocks.28.attn.proj.weight, vit_encoder.blocks.28.attn.qkv.bias, vit_encoder.blocks.28.attn.qkv.weight, vit_encoder.blocks.28.norm1.bias, vit_encoder.blocks.28.norm1.weight, vit_encoder.blocks.27.mlp.fc2.bias, vit_encoder.blocks.27.mlp.fc2.weight, vit_encoder.blocks.27.mlp.fc1.bias, vit_encoder.blocks.27.mlp.fc1.weight, vit_encoder.blocks.27.norm2.bias, vit_encoder.blocks.27.norm2.weight, vit_encoder.blocks.27.attn.proj.bias, vit_encoder.blocks.27.attn.proj.weight, vit_encoder.blocks.27.attn.qkv.bias, vit_encoder.blocks.27.attn.qkv.weight, vit_encoder.blocks.27.norm1.bias, vit_encoder.blocks.27.norm1.weight, vit_encoder.blocks.26.mlp.fc2.bias, vit_encoder.blocks.26.mlp.fc2.weight, vit_encoder.blocks.26.mlp.fc1.bias, vit_encoder.blocks.26.mlp.fc1.weight, vit_encoder.blocks.26.norm2.bias, vit_encoder.blocks.26.norm2.weight, vit_encoder.blocks.26.attn.proj.bias, vit_encoder.blocks.26.attn.proj.weight, vit_encoder.blocks.26.attn.qkv.bias, vit_encoder.blocks.26.attn.qkv.weight, vit_encoder.blocks.26.norm1.bias, vit_encoder.blocks.26.norm1.weight, vit_encoder.blocks.25.mlp.fc2.bias, vit_encoder.blocks.25.mlp.fc2.weight, vit_encoder.blocks.25.mlp.fc1.bias, vit_encoder.blocks.25.mlp.fc1.weight, vit_encoder.blocks.25.norm2.bias, vit_encoder.blocks.25.norm2.weight, vit_encoder.blocks.25.attn.proj.bias, vit_encoder.blocks.25.attn.proj.weight, vit_encoder.blocks.25.attn.qkv.bias, vit_encoder.blocks.25.attn.qkv.weight, vit_encoder.blocks.25.norm1.bias, vit_encoder.blocks.25.norm1.weight, vit_encoder.blocks.24.mlp.fc2.bias, vit_encoder.blocks.24.mlp.fc2.weight, vit_encoder.blocks.24.mlp.fc1.bias, vit_encoder.blocks.24.mlp.fc1.weight, vit_encoder.blocks.24.norm2.bias, vit_encoder.blocks.24.norm2.weight, vit_encoder.blocks.24.attn.proj.bias, vit_encoder.blocks.24.attn.proj.weight, vit_encoder.blocks.24.attn.qkv.bias, vit_encoder.blocks.24.attn.qkv.weight, vit_encoder.blocks.24.norm1.bias, vit_encoder.blocks.24.norm1.weight, vit_encoder.blocks.23.mlp.fc2.bias, vit_encoder.blocks.23.mlp.fc2.weight, vit_encoder.blocks.23.mlp.fc1.bias, vit_encoder.blocks.23.mlp.fc1.weight, vit_encoder.blocks.23.norm2.bias, vit_encoder.blocks.23.norm2.weight, vit_encoder.blocks.23.attn.proj.bias, vit_encoder.blocks.23.attn.proj.weight, vit_encoder.blocks.23.attn.qkv.bias, vit_encoder.blocks.23.attn.qkv.weight, vit_encoder.blocks.23.norm1.bias, vit_encoder.blocks.23.norm1.weight, vit_encoder.blocks.22.mlp.fc2.bias, vit_encoder.blocks.22.mlp.fc2.weight, vit_encoder.blocks.22.mlp.fc1.bias, vit_encoder.blocks.22.mlp.fc1.weight, vit_encoder.blocks.22.norm2.bias, vit_encoder.blocks.22.norm2.weight, vit_encoder.blocks.22.attn.proj.bias, vit_encoder.blocks.22.attn.proj.weight, vit_encoder.blocks.22.attn.qkv.bias, vit_encoder.blocks.22.attn.qkv.weight, vit_encoder.blocks.22.norm1.bias, vit_encoder.blocks.22.norm1.weight, vit_encoder.blocks.21.mlp.fc2.bias, vit_encoder.blocks.21.mlp.fc2.weight, vit_encoder.blocks.21.mlp.fc1.bias, vit_encoder.blocks.21.mlp.fc1.weight, vit_encoder.blocks.21.norm2.bias, vit_encoder.blocks.21.norm2.weight, vit_encoder.blocks.21.attn.proj.bias, vit_encoder.blocks.21.attn.proj.weight, vit_encoder.blocks.21.attn.qkv.bias, vit_encoder.blocks.21.attn.qkv.weight, vit_encoder.blocks.21.norm1.bias, vit_encoder.blocks.21.norm1.weight, vit_encoder.blocks.20.mlp.fc2.bias, vit_encoder.blocks.20.mlp.fc2.weight, vit_encoder.blocks.20.mlp.fc1.bias, vit_encoder.blocks.20.mlp.fc1.weight, vit_encoder.blocks.20.norm2.bias, vit_encoder.blocks.20.norm2.weight, vit_encoder.blocks.20.attn.proj.bias, vit_encoder.blocks.20.attn.proj.weight, vit_encoder.blocks.20.attn.qkv.bias, vit_encoder.blocks.20.attn.qkv.weight, vit_encoder.blocks.20.norm1.bias, vit_encoder.blocks.20.norm1.weight, vit_encoder.blocks.19.mlp.fc2.bias, vit_encoder.blocks.19.mlp.fc2.weight, vit_encoder.blocks.19.mlp.fc1.bias, vit_encoder.blocks.19.mlp.fc1.weight, vit_encoder.blocks.19.norm2.bias, vit_encoder.blocks.19.norm2.weight, vit_encoder.blocks.19.attn.proj.bias, vit_encoder.blocks.19.attn.proj.weight, vit_encoder.blocks.19.attn.qkv.bias, vit_encoder.blocks.19.attn.qkv.weight, vit_encoder.blocks.19.norm1.bias, vit_encoder.blocks.19.norm1.weight, vit_encoder.blocks.18.mlp.fc2.bias, vit_encoder.blocks.18.mlp.fc2.weight, vit_encoder.blocks.18.mlp.fc1.bias, vit_encoder.blocks.18.mlp.fc1.weight, vit_encoder.blocks.18.norm2.bias, vit_encoder.blocks.18.norm2.weight, vit_encoder.blocks.18.attn.proj.bias, vit_encoder.blocks.18.attn.proj.weight, vit_encoder.blocks.18.attn.qkv.bias, vit_encoder.blocks.18.attn.qkv.weight, vit_encoder.blocks.18.norm1.bias, vit_encoder.blocks.18.norm1.weight, vit_encoder.blocks.17.mlp.fc2.bias, vit_encoder.blocks.17.mlp.fc2.weight, vit_encoder.blocks.17.mlp.fc1.bias, vit_encoder.blocks.17.mlp.fc1.weight, vit_encoder.blocks.17.norm2.bias, vit_encoder.blocks.17.norm2.weight, vit_encoder.blocks.17.attn.proj.bias, vit_encoder.blocks.17.attn.proj.weight, vit_encoder.blocks.17.attn.qkv.bias, vit_encoder.blocks.17.attn.qkv.weight, vit_encoder.blocks.17.norm1.bias, vit_encoder.blocks.17.norm1.weight, vit_encoder.blocks.16.mlp.fc2.bias, vit_encoder.blocks.16.mlp.fc2.weight, vit_encoder.blocks.16.mlp.fc1.bias, vit_encoder.blocks.16.mlp.fc1.weight, vit_encoder.blocks.16.norm2.bias, vit_encoder.blocks.16.norm2.weight, vit_encoder.blocks.16.attn.proj.bias, vit_encoder.blocks.16.attn.proj.weight, vit_encoder.blocks.16.attn.qkv.bias, vit_encoder.blocks.16.attn.qkv.weight, vit_encoder.blocks.16.norm1.bias, vit_encoder.blocks.16.norm1.weight, vit_encoder.blocks.15.mlp.fc2.bias, vit_encoder.blocks.15.mlp.fc2.weight, vit_encoder.blocks.15.mlp.fc1.bias, vit_encoder.blocks.15.mlp.fc1.weight, vit_encoder.blocks.15.norm2.bias, vit_encoder.blocks.15.norm2.weight, vit_encoder.blocks.15.attn.proj.bias, vit_encoder.blocks.15.attn.proj.weight, vit_encoder.blocks.15.attn.qkv.bias, vit_encoder.blocks.15.attn.qkv.weight, vit_encoder.blocks.15.norm1.bias, vit_encoder.blocks.15.norm1.weight, vit_encoder.blocks.14.mlp.fc2.bias, vit_encoder.blocks.14.mlp.fc2.weight, vit_encoder.blocks.14.mlp.fc1.bias, vit_encoder.blocks.14.mlp.fc1.weight, vit_encoder.blocks.14.norm2.bias, vit_encoder.blocks.14.norm2.weight, vit_encoder.blocks.14.attn.proj.bias, vit_encoder.blocks.14.attn.proj.weight, vit_encoder.blocks.14.attn.qkv.bias, vit_encoder.blocks.14.attn.qkv.weight, vit_encoder.blocks.14.norm1.bias, vit_encoder.blocks.14.norm1.weight, vit_encoder.blocks.13.mlp.fc2.bias, vit_encoder.blocks.13.mlp.fc2.weight, vit_encoder.blocks.13.mlp.fc1.bias, vit_encoder.blocks.13.mlp.fc1.weight, vit_encoder.blocks.13.norm2.bias, vit_encoder.blocks.13.norm2.weight, vit_encoder.blocks.13.attn.proj.bias, vit_encoder.blocks.13.attn.proj.weight, vit_encoder.blocks.13.attn.qkv.bias, vit_encoder.blocks.13.attn.qkv.weight, vit_encoder.blocks.13.norm1.bias, vit_encoder.blocks.13.norm1.weight, vit_encoder.blocks.12.mlp.fc2.bias, vit_encoder.blocks.12.mlp.fc2.weight, vit_encoder.blocks.12.mlp.fc1.bias, vit_encoder.blocks.12.mlp.fc1.weight, vit_encoder.blocks.12.norm2.bias, vit_encoder.blocks.12.norm2.weight, vit_encoder.blocks.12.attn.proj.bias, vit_encoder.blocks.12.attn.proj.weight, vit_encoder.blocks.12.attn.qkv.bias, vit_encoder.blocks.12.attn.qkv.weight, vit_encoder.blocks.12.norm1.bias, vit_encoder.blocks.12.norm1.weight, vit_encoder.blocks.11.mlp.fc2.bias, vit_encoder.blocks.11.mlp.fc2.weight, vit_encoder.blocks.11.mlp.fc1.bias, vit_encoder.blocks.11.mlp.fc1.weight, vit_encoder.blocks.11.norm2.bias, vit_encoder.blocks.11.norm2.weight, vit_encoder.blocks.11.attn.proj.bias, vit_encoder.blocks.11.attn.proj.weight, vit_encoder.blocks.11.attn.qkv.bias, vit_encoder.blocks.11.attn.qkv.weight, vit_encoder.blocks.11.norm1.bias, vit_encoder.blocks.11.norm1.weight, vit_encoder.blocks.10.mlp.fc2.bias, vit_encoder.blocks.10.mlp.fc2.weight, vit_encoder.blocks.10.mlp.fc1.bias, vit_encoder.blocks.10.mlp.fc1.weight, vit_encoder.blocks.10.norm2.bias, vit_encoder.blocks.10.norm2.weight, vit_encoder.blocks.10.attn.proj.bias, vit_encoder.blocks.10.attn.proj.weight, vit_encoder.blocks.10.attn.qkv.bias, vit_encoder.blocks.10.attn.qkv.weight, vit_encoder.blocks.10.norm1.bias, vit_encoder.blocks.10.norm1.weight, vit_encoder.blocks.9.mlp.fc2.bias, vit_encoder.blocks.9.mlp.fc2.weight, vit_encoder.blocks.9.mlp.fc1.bias, vit_encoder.blocks.9.mlp.fc1.weight, vit_encoder.blocks.9.norm2.bias, vit_encoder.blocks.9.norm2.weight, vit_encoder.blocks.9.attn.proj.bias, vit_encoder.blocks.9.attn.proj.weight, vit_encoder.blocks.9.attn.qkv.bias, vit_encoder.blocks.9.attn.qkv.weight, vit_encoder.blocks.9.norm1.bias, vit_encoder.blocks.9.norm1.weight, vit_encoder.blocks.8.mlp.fc2.bias, vit_encoder.blocks.8.mlp.fc2.weight, vit_encoder.blocks.8.mlp.fc1.bias, vit_encoder.blocks.8.mlp.fc1.weight, vit_encoder.blocks.8.norm2.bias, vit_encoder.blocks.8.norm2.weight, vit_encoder.blocks.8.attn.proj.bias, vit_encoder.blocks.8.attn.proj.weight, vit_encoder.blocks.8.attn.qkv.bias, vit_encoder.blocks.8.attn.qkv.weight, vit_encoder.blocks.8.norm1.bias, vit_encoder.blocks.8.norm1.weight, vit_encoder.blocks.7.mlp.fc2.bias, vit_encoder.blocks.7.mlp.fc2.weight, vit_encoder.blocks.7.mlp.fc1.bias, vit_encoder.blocks.7.mlp.fc1.weight, vit_encoder.blocks.7.norm2.bias, vit_encoder.blocks.7.norm2.weight, vit_encoder.blocks.7.attn.proj.bias, vit_encoder.blocks.7.attn.proj.weight, vit_encoder.blocks.7.attn.qkv.bias, vit_encoder.blocks.7.attn.qkv.weight, vit_encoder.blocks.7.norm1.bias, vit_encoder.blocks.7.norm1.weight, vit_encoder.blocks.6.mlp.fc2.bias, vit_encoder.blocks.6.mlp.fc2.weight, vit_encoder.blocks.6.mlp.fc1.bias, vit_encoder.blocks.6.mlp.fc1.weight, vit_encoder.blocks.6.norm2.bias, vit_encoder.blocks.6.norm2.weight, vit_encoder.blocks.6.attn.proj.bias, vit_encoder.blocks.6.attn.proj.weight, vit_encoder.blocks.6.attn.qkv.bias, vit_encoder.blocks.6.attn.qkv.weight, vit_encoder.blocks.6.norm1.bias, vit_encoder.blocks.6.norm1.weight, vit_encoder.blocks.5.mlp.fc2.bias, vit_encoder.blocks.5.mlp.fc2.weight, vit_encoder.blocks.5.mlp.fc1.bias, vit_encoder.blocks.5.mlp.fc1.weight, vit_encoder.blocks.5.norm2.bias, vit_encoder.blocks.5.norm2.weight, vit_encoder.blocks.5.attn.proj.bias, vit_encoder.blocks.5.attn.proj.weight, vit_encoder.blocks.5.attn.qkv.bias, vit_encoder.blocks.5.attn.qkv.weight, vit_encoder.blocks.5.norm1.bias, vit_encoder.blocks.5.norm1.weight, vit_encoder.blocks.4.mlp.fc2.bias, vit_encoder.blocks.4.mlp.fc2.weight, vit_encoder.blocks.4.mlp.fc1.bias, vit_encoder.blocks.4.mlp.fc1.weight, vit_encoder.blocks.4.norm2.bias, vit_encoder.blocks.4.norm2.weight, vit_encoder.blocks.4.attn.proj.bias, vit_encoder.blocks.4.attn.proj.weight, vit_encoder.blocks.4.attn.qkv.bias, vit_encoder.blocks.4.attn.qkv.weight, vit_encoder.blocks.4.norm1.bias, vit_encoder.blocks.4.norm1.weight, vit_encoder.blocks.3.mlp.fc2.bias, vit_encoder.blocks.3.mlp.fc2.weight, vit_encoder.blocks.3.mlp.fc1.bias, vit_encoder.blocks.3.mlp.fc1.weight, vit_encoder.blocks.3.norm2.bias, vit_encoder.blocks.3.norm2.weight, vit_encoder.blocks.3.attn.proj.bias, vit_encoder.blocks.3.attn.proj.weight, vit_encoder.blocks.3.attn.qkv.bias, vit_encoder.blocks.3.attn.qkv.weight, vit_encoder.blocks.3.norm1.bias, vit_encoder.blocks.3.norm1.weight, vit_encoder.blocks.2.mlp.fc2.bias, vit_encoder.blocks.2.mlp.fc2.weight, vit_encoder.blocks.2.mlp.fc1.bias, vit_encoder.blocks.2.mlp.fc1.weight, vit_encoder.blocks.2.norm2.bias, vit_encoder.blocks.2.norm2.weight, vit_encoder.blocks.2.attn.proj.bias, vit_encoder.blocks.2.attn.proj.weight, vit_encoder.blocks.2.attn.qkv.bias, vit_encoder.blocks.2.attn.qkv.weight, vit_encoder.blocks.2.norm1.bias, vit_encoder.blocks.2.norm1.weight, vit_encoder.blocks.1.mlp.fc2.bias, vit_encoder.blocks.1.mlp.fc2.weight, vit_encoder.blocks.1.mlp.fc1.bias, vit_encoder.blocks.1.mlp.fc1.weight, vit_encoder.blocks.1.norm2.bias, vit_encoder.blocks.1.norm2.weight, vit_encoder.blocks.1.attn.proj.bias, vit_encoder.blocks.1.attn.proj.weight, vit_encoder.blocks.1.attn.qkv.bias, vit_encoder.blocks.1.attn.qkv.weight, vit_encoder.blocks.1.norm1.bias, vit_encoder.blocks.1.norm1.weight, vit_encoder.blocks.0.mlp.fc2.bias, vit_encoder.blocks.0.mlp.fc2.weight, vit_encoder.blocks.0.mlp.fc1.bias, vit_encoder.blocks.0.mlp.fc1.weight, vit_encoder.blocks.0.norm2.bias, vit_encoder.blocks.0.norm2.weight, vit_encoder.blocks.0.attn.proj.bias, vit_encoder.blocks.0.attn.proj.weight, vit_encoder.blocks.0.attn.qkv.bias, vit_encoder.blocks.0.attn.qkv.weight, vit_encoder.blocks.0.norm1.bias, vit_encoder.blocks.0.norm1.weight, vit_encoder.patch_embed.proj.bias, vit_encoder.patch_embed.proj.weight, vit_encoder.pos_embed
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 ...
