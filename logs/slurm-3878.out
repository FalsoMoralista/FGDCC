INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:root:called-params configs/in1k_vith14_ep300_FGDCC.yaml
INFO:root:loaded params...
{   'data': {   'batch_size': 96,
                'color_jitter_strength': 0.0,
                'crop_scale': [0.3, 1.0],
                'crop_size': 224,
                'cutmix': 0.0,
                'drop_path': 0.25,
                'image_folder': '/home/rtcalumby/adam/luciano/plantnet_300K/',
                'mixup': 0.0,
                'nb_classes': 1081,
                'num_workers': 16,
                'pin_mem': True,
                'reprob': 0.25,
                'resume_epoch': 0,
                'root_path': '/home/rtcalumby/adam/luciano/',
                'use_color_distortion': False,
                'use_gaussian_blur': False,
                'use_horizontal_flip': False},
    'logging': {   'folder': '/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/logs/PlantNet300k_exp40',
                   'write_tag': 'jepa'},
    'mask': {   'allow_overlap': False,
                'aspect_ratio': [0.75, 1.5],
                'enc_mask_scale': [0.85, 1.0],
                'min_keep': 10,
                'num_enc_masks': 1,
                'num_pred_masks': 4,
                'patch_size': 14,
                'pred_mask_scale': [0.15, 0.2]},
    'meta': {   'copy_data': False,
                'load_checkpoint': True,
                'model_name': 'vit_huge',
                'pred_depth': 12,
                'pred_emb_dim': 384,
                'read_checkpoint': None,
                'use_bfloat16': True},
    'optimization': {   'ema': [0.9, 0.999],
                        'epochs': 75,
                        'final_lr': 1e-05,
                        'final_weight_decay': 0.4,
                        'ipe_scale': 1.0,
                        'label_smoothing': 0.1,
                        'lr': 2.5e-05,
                        'start_lr': 8.5e-05,
                        'warmup': 5,
                        'weight_decay': 0.05}}
INFO:root:Running... (rank: 0/1)
INFO:root:Initialized (rank/world-size) 0/1
INFO:root:MaskedAutoEncoder(
  (encoder): Sequential(
    (0): Linear(in_features=1280, out_features=1152, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=1152, out_features=1024, bias=True)
    (3): GELU(approximate='none')
    (4): Linear(in_features=1024, out_features=768, bias=True)
  )
  (decoder): Sequential(
    (0): Linear(in_features=768, out_features=1024, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=1024, out_features=1152, bias=True)
    (3): GELU(approximate='none')
    (4): Linear(in_features=1152, out_features=1280, bias=True)
  )
)
INFO:root:making imagenet data transforms
INFO:root:making imagenet data transforms
INFO:root:Finetuning dataset created
Training dataset, length: 245952
INFO:root:Finetuning dataset created
Val dataset, length: 31200
INFO:root:Using AdamW
['encoder', 'predictor', 'opt', 'scaler', 'target_encoder', 'epoch', 'loss', 'batch_size', 'world_size', 'lr']
INFO:root:loaded pretrained encoder from epoch 66 with msg: <All keys matched successfully>
INFO:root:PairedCrossAttentionClassifier(
  (act): GELU(approximate='none')
  (subclass_proj): Sequential(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): GELU(approximate='none')
  )
  (parent_cross_attention): MultiHeadCrossAttention(
    (query): Linear(in_features=1280, out_features=1280, bias=True)
    (key): Linear(in_features=1280, out_features=1280, bias=True)
    (value): Linear(in_features=1280, out_features=1280, bias=True)
  )
  (subclass_cross_attention): MultiHeadCrossAttention(
    (query): Linear(in_features=1280, out_features=1280, bias=True)
    (key): Linear(in_features=1280, out_features=1280, bias=True)
    (value): Linear(in_features=1280, out_features=1280, bias=True)
  )
  (parent_feature_selection): Linear(in_features=2560, out_features=1280, bias=True)
  (subclass_feature_selection): Linear(in_features=2560, out_features=1280, bias=True)
  (parent_classifier): Linear(in_features=1280, out_features=1081, bias=True)
  (subclass_classifier): Linear(in_features=1280, out_features=4324, bias=True)
  (head_drop): Dropout(p=0.25, inplace=False)
)
INFO:root:Using AdamW
INFO:root:VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14))
  )
  (blocks): ModuleList(
    (0-31): 32 x Block(
      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1280, out_features=3840, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1280, out_features=1280, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=1280, out_features=5120, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=5120, out_features=1280, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
)
INFO:root:SpatialMaskedAutoEncoder(
  (encoder): Sequential(
    (0): Linear(in_features=1280, out_features=1152, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=1152, out_features=1024, bias=True)
    (3): GELU(approximate='none')
    (4): Linear(in_features=1024, out_features=960, bias=True)
    (5): GELU(approximate='none')
    (6): Linear(in_features=960, out_features=896, bias=True)
    (7): GELU(approximate='none')
    (8): Linear(in_features=896, out_features=832, bias=True)
    (9): GELU(approximate='none')
    (10): Linear(in_features=832, out_features=768, bias=True)
  )
  (decoder): Sequential(
    (0): Linear(in_features=768, out_features=832, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=832, out_features=896, bias=True)
    (3): GELU(approximate='none')
    (4): Linear(in_features=896, out_features=960, bias=True)
    (5): GELU(approximate='none')
    (6): Linear(in_features=960, out_features=1024, bias=True)
    (7): GELU(approximate='none')
    (8): Linear(in_features=1024, out_features=1152, bias=True)
    (9): GELU(approximate='none')
    (10): Linear(in_features=1152, out_features=1280, bias=True)
  )
)
INFO:root:Autoencoder Training...
INFO:root: - - Epoch: 1 - - 
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:root:[1,     0/ 2562] - [Autoencoder Training] [Autoencoder Loss: 1.9450] [autoencoder lr: 1.00e-04][mem: 8.40e+03](2639.3 ms)
INFO:root:[1,   100/ 2562] - [Autoencoder Training] [Autoencoder Loss: 1.9452] [autoencoder lr: 1.00e-04][mem: 8.90e+03](352.7 ms)
INFO:root:[1,   200/ 2562] - [Autoencoder Training] [Autoencoder Loss: 1.9454] [autoencoder lr: 1.01e-04][mem: 8.90e+03](342.0 ms)
INFO:root:[1,   300/ 2562] - [Autoencoder Training] [Autoencoder Loss: 1.9451] [autoencoder lr: 1.01e-04][mem: 8.90e+03](338.7 ms)
INFO:root:[1,   400/ 2562] - [Autoencoder Training] [Autoencoder Loss: 1.9450] [autoencoder lr: 1.02e-04][mem: 8.90e+03](337.1 ms)
INFO:root:[1,   500/ 2562] - [Autoencoder Training] [Autoencoder Loss: 1.9450] [autoencoder lr: 1.02e-04][mem: 8.90e+03](336.4 ms)
INFO:root:[1,   600/ 2562] - [Autoencoder Training] [Autoencoder Loss: 1.9451] [autoencoder lr: 1.02e-04][mem: 8.90e+03](335.8 ms)
INFO:root:[1,   700/ 2562] - [Autoencoder Training] [Autoencoder Loss: 1.9451] [autoencoder lr: 1.03e-04][mem: 8.90e+03](335.2 ms)
INFO:root:[1,   800/ 2562] - [Autoencoder Training] [Autoencoder Loss: 1.9451] [autoencoder lr: 1.03e-04][mem: 8.90e+03](334.8 ms)
INFO:root:[1,   900/ 2562] - [Autoencoder Training] [Autoencoder Loss: 1.9451] [autoencoder lr: 1.04e-04][mem: 8.90e+03](334.5 ms)
INFO:root:[1,  1000/ 2562] - [Autoencoder Training] [Autoencoder Loss: 1.9451] [autoencoder lr: 1.04e-04][mem: 8.90e+03](334.3 ms)
INFO:root:[1,  1100/ 2562] - [Autoencoder Training] [Autoencoder Loss: 1.9450] [autoencoder lr: 1.04e-04][mem: 8.90e+03](334.2 ms)
INFO:root:[1,  1200/ 2562] - [Autoencoder Training] [Autoencoder Loss: 1.9451] [autoencoder lr: 1.05e-04][mem: 8.90e+03](333.9 ms)
INFO:root:[1,  1300/ 2562] - [Autoencoder Training] [Autoencoder Loss: 1.9451] [autoencoder lr: 1.05e-04][mem: 8.90e+03](333.8 ms)
INFO:root:[1,  1400/ 2562] - [Autoencoder Training] [Autoencoder Loss: 1.9451] [autoencoder lr: 1.05e-04][mem: 8.90e+03](333.7 ms)
INFO:root:[1,  1500/ 2562] - [Autoencoder Training] [Autoencoder Loss: 1.9451] [autoencoder lr: 1.06e-04][mem: 8.90e+03](333.7 ms)
INFO:root:[1,  1600/ 2562] - [Autoencoder Training] [Autoencoder Loss: 1.9451] [autoencoder lr: 1.06e-04][mem: 8.90e+03](333.6 ms)
INFO:root:[1,  1700/ 2562] - [Autoencoder Training] [Autoencoder Loss: 1.9451] [autoencoder lr: 1.07e-04][mem: 8.90e+03](333.4 ms)
INFO:root:[1,  1800/ 2562] - [Autoencoder Training] [Autoencoder Loss: 1.9451] [autoencoder lr: 1.07e-04][mem: 8.90e+03](333.3 ms)
INFO:root:[1,  1900/ 2562] - [Autoencoder Training] [Autoencoder Loss: 1.9452] [autoencoder lr: 1.07e-04][mem: 8.90e+03](333.1 ms)
INFO:root:[1,  2000/ 2562] - [Autoencoder Training] [Autoencoder Loss: 1.9452] [autoencoder lr: 1.08e-04][mem: 8.90e+03](333.1 ms)
INFO:root:[1,  2100/ 2562] - [Autoencoder Training] [Autoencoder Loss: 1.9452] [autoencoder lr: 1.08e-04][mem: 8.90e+03](333.0 ms)
INFO:root:[1,  2200/ 2562] - [Autoencoder Training] [Autoencoder Loss: 1.9451] [autoencoder lr: 1.09e-04][mem: 8.90e+03](332.9 ms)
INFO:root:[1,  2300/ 2562] - [Autoencoder Training] [Autoencoder Loss: 1.9451] [autoencoder lr: 1.09e-04][mem: 8.90e+03](332.8 ms)
INFO:root:[1,  2400/ 2562] - [Autoencoder Training] [Autoencoder Loss: 1.9451] [autoencoder lr: 1.09e-04][mem: 8.90e+03](332.8 ms)
INFO:root:[1,  2500/ 2562] - [Autoencoder Training] [Autoencoder Loss: 1.9451] [autoencoder lr: 1.10e-04][mem: 8.90e+03](332.7 ms)
INFO:root:Initializing centroids...
INFO:root:Update Step...
/home/rtcalumby/miniconda3/envs/py382/lib/python3.12/site-packages/faiss/contrib/torch_utils.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  x.storage().data_ptr() + x.storage_offset() * 4)
INFO:root:Epoch 1
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/home/rtcalumby/miniconda3/envs/py382/lib/python3.12/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rtcalumby/miniconda3/envs/py382/lib/python3.12/multiprocessing/spawn.py", line 131, in _main
    prepare(preparation_data)
  File "/home/rtcalumby/miniconda3/envs/py382/lib/python3.12/multiprocessing/spawn.py", line 246, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/home/rtcalumby/miniconda3/envs/py382/lib/python3.12/multiprocessing/spawn.py", line 297, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen runpy>", line 286, in run_path
  File "<frozen runpy>", line 98, in _run_module_code
  File "<frozen runpy>", line 88, in _run_code
  File "/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/DeepCluster/main_FGDCC.py", line 16, in <module>
    from engine_FGDCC import main as app_main
  File "/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/DeepCluster/engine_FGDCC.py", line 61, in <module>
    from src.models import FGDCC
  File "/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/DeepCluster/src/models/FGDCC.py", line 8, in <module>
    from src.models.transformer_autoencoder import SpatialMaskedAutoEncoder
  File "/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/DeepCluster/src/models/transformer_autoencoder.py", line 9, in <module>
    from vision_transformer import Block
ModuleNotFoundError: No module named 'vision_transformer'
slurmstepd: error: *** JOB 3878 ON hgx CANCELLED AT 2024-11-10T14:32:18 ***
